---
title: "Vector representations to cosine similarities"
output: html_document
date: "2024-02-09"
---

```{r setup, include=FALSE,warning=F,message=F}
knitr::opts_chunk$set(echo = TRUE, error = T)

# Libraries and Settings

# Libs ---------------------------
library(tidyverse)
library(arrow)
library(glue)
library(psych)
library(lavaan)
library(ggplot2)
library(plotly)
library(gridExtra)

model_name = "ItemSimilarityTraining-20240502-trial12"
#model_name = "item-similarity-20231018-122504"
pretrained_model_name = "all-mpnet-base-v2"

data_path = glue("./")
pretrained_data_path = glue("./")

set.seed(42)
source("global_functions.R")
```


## Load data
```{r}
rr_validation_mapping_data = arrow::read_feather(
  file = "https://github.com/synth-science/surveybot3000/raw/refs/heads/main/validation_study/mapping.feather"
)

items <- rio::import("https://docs.google.com/spreadsheets/d/16QcRLP5BUn1Cmtr0e_XRdjr1Wg-EHSMSGmgZO1M3tNM/edit?gid=0#gid=0", which = 2)

scales <- rr_validation_mapping_data %>% 
  select(-scale0, -scale1) %>% 
  left_join(items %>% 
              filter(in_survey) %>% 
              mutate(keyed = if_else(reversed, -1, 1)) %>% 
              select(id, keyed, scale, subscale), by = c("variable" = "id")) %>% 
  rename(scale_0 = scale, scale_1 = subscale) %>% 
  mutate(scale_1 = if_else(scale_1 == "#N/A", "", scale_1))

rr_validation_mapping_data <- scales %>% 
  rename(scale0 = scale_0, scale1 = scale_1)

scales <- bind_rows(
  scales %>% 
      mutate(
        scale_1 = "",
         scale = str_replace_all(str_trim(paste0(instrument, "_", scale_0, scale_1)), "[^a-zA-Z_0-9]", "_")
  ) %>% 
    group_by(scale) %>% 
    mutate(number_of_items = n()),
  scales %>% 
    mutate(
      scale_1 = coalesce(scale_1, ""),
      scale = str_replace_all(str_trim(paste0(instrument, "_", scale_0, scale_1)), "[^a-zA-Z_0-9]", "_")) %>% 
    group_by(scale) %>% 
    mutate(number_of_items = n())
  ) %>% 
  group_by(scale) %>% 
  filter(row_number() == 1) %>% 
  ungroup() %>% 
  select(-variable, -item_text)

arrow::write_feather(scales, sink = file.path(data_path, glue("{model_name}.raw.validation-study-2024-11-01.scales.feather"))
)

rr_validation_mapping_data <- rr_validation_mapping_data %>% 
  mutate(scale0 = str_replace(scale0, "opennness", "openness"))

arrow::write_feather(rr_validation_mapping_data, sink = file.path(data_path, glue("{model_name}.raw.validation-study-2024-11-01.mapping2.feather"))
)


# pre-trained model
pt_rr_validation_machine_data = rio::import("https://github.com/synth-science/surveybot3000/raw/refs/heads/main/validation_study/embeddings_all-mpnet-base-v2.feather")
pt_rr_validation_machine_data <- pt_rr_validation_machine_data %>% 
  rowwise() %>% 
  mutate(embed_id = list(1:length(embeddings))) %>% 
  unnest(cols = c(embeddings, embed_id)) %>% 
  select(-item) %>% 
  pivot_wider(names_from = id, values_from = embeddings) %>% 
  select(-embed_id)

# fine-tuned model
rr_validation_machine_data = rio::import("https://github.com/synth-science/surveybot3000/raw/refs/heads/main/validation_study/embeddings_surveybot3000.feather")
rr_validation_machine_data <- rr_validation_machine_data %>% 
  rowwise() %>% 
  mutate(embed_id = list(1:length(embeddings))) %>% 
  unnest(cols = c(embeddings, embed_id)) %>% 
  select(-item) %>% 
  pivot_wider(names_from = id, values_from = embeddings) %>% 
  select(-embed_id)
  
main_qs <- c("AAID", "PANAS", "PAQ", "PSS", "NEPS", "ULS", "FCV", "DAQ", "CESD", "HEXACO", "OCIR", "PTQ", "RAAS", "KSA", "SAS", "MFQ", "CQ", "OLBI", "UWES", "WGS")
rr_validation_human_data = rio::import("../synth-rep-dataset/data/processed/sosci_labelled_with_exclusion_criteria.rds") %>% filter(included) %>% 
	select(starts_with(main_qs)) %>% 
	select(-ends_with("_R"))

rio::export(rr_validation_human_data, file = file.path(data_path, glue("{model_name}.raw.validation-study-2024-11-01.human.feather"))
)


setdiff(colnames(rr_validation_human_data), rr_validation_mapping_data$variable)
setdiff(rr_validation_mapping_data$variable, colnames(rr_validation_human_data))
```

## Description
```{r}
nrow(rr_validation_human_data) # respondents
ncol(rr_validation_human_data) # items
nrow(rr_validation_machine_data) # vector dimensions
ncol(rr_validation_machine_data) # items
n_distinct(rr_validation_mapping_data$instrument) # instruments
n_distinct(rr_validation_mapping_data$scale0) # constructs
n_distinct(rr_validation_mapping_data$instrument, rr_validation_mapping_data$scale0) # scales
n_distinct(str_c(rr_validation_mapping_data$instrument, rr_validation_mapping_data$scale0, rr_validation_mapping_data$scale1)) # subscales
scales %>% tally()
scales %>% filter(number_of_items > 2) %>% tally()
```


## Join pairwise item correlations
```{r}
join_pairwise_correlation = function(df_human, df_machine) {
    item_pairs = combn(x = names(df_human), m = 2) %>%
        t() %>% 
      as.data.frame()
    colnames(item_pairs) <- c("variable_1", "variable_2")

    df_  = 
      item_pairs %>% 
      left_join(
      df_human %>% 
        cor(use = "p") %>% 
        reshape2::melt() %>% 
        dplyr::left_join(
          Hmisc::rcorr(as.matrix(rr_validation_human_data))$n %>% 
          reshape2::melt() %>% 
          dplyr::rename(pairwise_n = value),
          by = c("Var1", "Var2")
        ) %>% 
        dplyr::left_join(
            y = df_machine %>% 
            cor(use = "p") %>% 
            reshape2::melt(),
            by = c("Var1", "Var2")
        ) %>% 
        dplyr::rename(
            empirical_r = "value.x",
            synthetic_r = "value.y",
            variable_1 = Var1,
            variable_2 = Var2
        ) %>% 
        dplyr::filter(variable_1 != variable_2),
    by = c("variable_1", "variable_2"))

    df_ <- df_ %>% 
      dplyr::mutate(empirical_r_se = (1 - empirical_r^2)/sqrt(pairwise_n - 3))
    return(df_)
}

rr_validation_item_pairs = join_pairwise_correlation(rr_validation_human_data, rr_validation_machine_data)

arrow::write_feather(rr_validation_item_pairs, sink = file.path(data_path, glue("ignore.{model_name}.raw.validation-study-2024-11-01.item_correlations.feather")))

pt_rr_validation_item_pairs = join_pairwise_correlation(rr_validation_human_data, pt_rr_validation_machine_data)

arrow::write_feather(pt_rr_validation_item_pairs, sink = file.path(data_path, glue("ignore.{pretrained_model_name}.raw.validation-study-2024-11-01.item_correlations.feather")))
```


## Join pairwise scale correlations
```{r}
predict_manifest_scores = function(human_data, machine_data, mapping_data, scale_data) {
  human_data <- human_data %>% 
    haven::zap_labels()
  human_cor = human_data %>%
    cor(use = "p")

  machine_cor = machine_data %>%
    cor(use = "p")

  mapping_data <- mapping_data %>% 
                             rename(scale_0 = scale0,
                                    scale_1 = scale1)
  scale_data <- scale_data %>% select(-keyed)
  items_by_scale <- bind_rows(
    scale_data %>% filter(scale_1 == "") %>% left_join(mapping_data %>% select(-scale_1), by = c("instrument", "scale_0")),
    scale_data %>% filter(scale_1 != "") %>% left_join(mapping_data, by = c("instrument", "scale_0", "scale_1"))
  )

  scale_data = items_by_scale %>%
    dplyr::group_by(scale) %>%
    dplyr::summarize(
      items = list(variable),
      reverse_keyed_items = list(variable[keyed == -1])
    )


  scale_pairs = combn(x = scale_data$scale, m = 2) %>%
    t() %>% 
    as_tibble()
  
  # no pairs between subscales and their parents
  scale_pairs <- scale_pairs %>% 
    filter(! str_detect(V1, fixed(V2))) %>% 
    filter(! str_detect(V2, fixed(V1))) %>% 
    as.matrix()

  manifest_scores = tibble()

  calculate_row_means = function(data_, scale_data_) {
    data_ %>%
      dplyr::select(
        scale_data_$items %>%
          unlist() %>%
          dplyr::all_of()
      ) %>%
      dplyr::mutate_at(
        .vars = scale_data_$reverse_keyed_items %>%
          unlist(),
        .funs = function(x) max(., na.rm = TRUE) + 1 - x
      ) %>%
      rowMeans(na.rm = TRUE)
  }

  for (i in seq_len(nrow(scale_pairs))) {
    scale_a = scale_pairs[i, 1]
    scale_b = scale_pairs[i, 2]

    scale_data_a = scale_data %>%
      dplyr::filter(scale == scale_a)

    scale_data_b = scale_data %>%
      dplyr::filter(scale == scale_b)

    human_a = calculate_row_means(human_data, scale_data_a)
    human_b = calculate_row_means(human_data, scale_data_b)
    machine_a = calculate_row_means(machine_data, scale_data_a)
    machine_b = calculate_row_means(machine_data, scale_data_b)

    human_cor <- broom::tidy(cor.test(human_a, human_b))
    manifest_scores = manifest_scores %>%
      dplyr::bind_rows(
        tibble(
          scale_a = scale_a,
          scale_b = scale_b,
          empirical_r = human_cor$estimate,
          pairwise_n = human_cor$parameter + 2,
          empirical_r_se = (1 - empirical_r^2)/sqrt(pairwise_n - 3),
          synthetic_r = cor(machine_a, machine_b, use = "p")
        )
      )
  }
  return(manifest_scores)
}


manifest_scores = predict_manifest_scores(rr_validation_human_data, rr_validation_machine_data, rr_validation_mapping_data, scales)

arrow::write_feather(manifest_scores, sink = file.path(data_path, glue("ignore.{model_name}.raw.validation-study-2024-11-01.scale_correlations.feather")))

pt_manifest_scores = predict_manifest_scores(rr_validation_human_data, pt_rr_validation_machine_data, rr_validation_mapping_data, scales)

arrow::write_feather(pt_manifest_scores, sink = file.path(data_path, glue("ignore.{pretrained_model_name}.raw.validation-study-2024-11-01.scale_correlations.feather")))
```


