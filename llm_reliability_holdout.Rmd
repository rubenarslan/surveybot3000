---
title: "LLM Reliability"
author: "Ruben Arslan"
date: "2023-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(lavaan)
library(semTools)
library(plotly)
```

```{r}
holdout <- arrow::read_feather("ignore.data-holdout-set-item-similarity-20230710-164559")
llm_holdout_meta <- arrow::read_feather("ignore.llmdata-holdout-set-item-similarity-20230710-164559.feather")
holdout_real <- holdout %>% 
  select(ItemStemIdA, ItemStemIdB, Pearson) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdA = ItemStemId, VariableA = Variable, InstrumentA = instrument, ScaleA = scale_0, SubscaleA = scale_1)) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdB = ItemStemId, VariableB = Variable, InstrumentB = instrument, ScaleB = scale_0, SubscaleB = scale_1))

holdout_llm <- holdout %>% 
  select(ItemStemIdA, ItemStemIdB, CosineSimilarity) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdA = ItemStemId, VariableA = Variable, InstrumentA = instrument, ScaleA = scale_0, SubscaleA = scale_1)) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdB = ItemStemId, VariableB = Variable, InstrumentB = instrument, ScaleB = scale_0, SubscaleB = scale_1))

cors_llm <- holdout_llm %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = CosineSimilarity) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_llm) <- 1

cors_real <- holdout_real %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = Pearson) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_real) <- 1

scales <- llm_holdout_meta %>% 
  drop_na(instrument, scale_0) %>% 
  mutate(scale = str_replace_all(paste(instrument, scale_0), "[^a-zA-Z_0-9]", "_")) %>% 
  group_by(scale) %>% 
  summarise(
    items = list(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>% 
  drop_na()

random_scales <- list()
for(i in 1:100) {
  n_items <- rpois(1, 10)
  random_scales[[i]] <- llm_holdout_meta %>% 
    drop_na(instrument, scale_0) %>% 
    sample_n(n_items) %>%  
    mutate(scale = paste0("random", i)) %>% 
  group_by(scale) %>% 
  summarise(
    items = list(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>% 
  drop_na()
}
random_scales <- bind_rows(random_scales)
scales <- bind_rows(scales, random_scales)

find_reverse_items <- function(rs) {
  # Calculate the mean correlation for each item, excluding the item's correlation with itself.
  mean_correlations <- apply(rs, 1, function(x) mean(x[-which(x == 1)]))

  # Identify items with negative mean correlation
  # You may adjust the threshold according to your specific criteria.
  threshold <- -0.01
  reverse_keyed_items <- names(which(mean_correlations < threshold))

  # Now you know which items are likely to be reverse-coded.  
  reverse_keyed_items
}

find_reverse_items_by_first_item <- function(rs) {
  # negatively correlated with first item
  items <- rs[-1, 1]
  reverse_keyed_items <- names(items)[which(items < 0)]
  reverse_keyed_items
}

reverse_items <- function(rs, reverse_keyed_items) {
  # Reverse the correlations for the reverse-keyed items
  for (item in reverse_keyed_items) {
    # Get the index of the reverse-keyed item
    item_index <- which(rownames(rs) == item)
    
    # Reverse the correlations
    rs[item_index, ] <- rs[item_index, ] * -1
    rs[, item_index] <- rs[, item_index] * -1
    
    # Since the diagonal is the correlation of the item with itself, set it back to 1
    rs[item_index, item_index] <- 1
  }
  rs
}


scales <- scales %>% 
  rowwise() %>% 
  mutate(r_real = list(cors_real[items, items]),
         r_llm = list(cors_llm[items, items])) %>% 
  mutate(reverse_items = list(find_reverse_items_by_first_item(r_real)),
         r_real_rev = list(reverse_items(r_real, reverse_items)),
         r_llm_rev = list(reverse_items(r_llm, reverse_items))) %>% 
  mutate(
    cfa_real = list(cfa(lvn, sample.cov = r_real_rev, sample.nobs = 1000)),
    rel_real = semTools::compRelSEM(cfa_real)) %>%
  mutate(
    cfa_llm = list(cfa(lvn, sample.cov = r_llm_rev, sample.nobs = 1000)),
    rel_llm = semTools::compRelSEM(cfa_llm))

scales <- scales %>% filter(between(as.vector(rel_llm), 0, 1)) %>% filter(between(as.vector(rel_real), 0, 1))

cor.test(scales$rel_llm, scales$rel_real)
# scales %>% filter(scale == "LOT_Optimism") %>% pull(cfa_real) %>% .[[1]] %>% summary
```



```{r}
holdout <- holdout %>% 
  mutate(se = (1 - Pearson^2)/sqrt(340 - 2),
         rel = 1 - (se^2/(var(Pearson) + se^2))
        )
mean(holdout$se^2)
cor(holdout$Pearson, holdout$CosineSimilarity)
summary(lm(Pearson ~ CosineSimilarity, holdout))
cor(holdout$Pearson, holdout$CosineSimilarity)/sqrt(mean(holdout$rel))

mean(holdout$rel)
corr_l <- cfa(
"LPearson ~~ CosineSimilarity
LPearson =~ Pearson
LPearson ~~ 0.90*LPearson", holdout)
summary(corr_l, std=T)

library(lavaan)

model <- '
  # Latent variables
  PearsonLatent =~ 1*Pearson

  # Fixing error variances based on known standard errors
  Pearson ~~ 0.002807621*Pearson

  # Relationship between latent variables
  PearsonLatent ~~ CosineSimilarity
'

fit <- sem(model, data = holdout %>% mutate(se2 = se^2))
summary(fit, std=T)


library(brms)
(m3 <- brm(bf(Pearson | mi(se) ~ 1) +
          bf(CosineSimilarity ~ 1) + set_rescor(T), data = holdout, backend = "cmdstanr", cores = 4))

holdout %>% summarise(mean(se))
corr <- cfa(
"Pearson ~~ CosineSimilarity", holdout)
summary(corr, std=T)
corr_l <- cfa(
"LPearson ~~ CosineSimilarity
LPearson =~ Pearson
LPearson ~~ 0.0529*LPearson", holdout)
summary(corr_l, std=T)
broom::tidy(cor.test(holdout$CosineSimilarity, holdout$Pearson))


holdout %>% 
  mutate(items = str_c(ItemStemTextA, "\n", ItemStemTextB)) %>% 
ggplot(., aes(CosineSimilarity, Pearson, label = items)) + 
  geom_abline(linetype = "dashed") +
  geom_point(size = 0.05, alpha = 0.01) +
  scale_color_viridis_d(guide = "none", end = 0.8) +
  ylab("Actual correlation") +
  theme_bw() +
  xlab("LLM cosine similarity") +
  ggtitle("Predicting correlations from LLMs, r = 0.7") +
  guides(color = "none") +
  coord_fixed(xlim = c(-1,1), ylim = c(-1,1)) -> p

p
ggplotly(p)
```


```{r}

summary(lm(rel_real ~ rel_llm, data = scales %>% filter(!str_detect(scale, "random"))))

scales %>% 
  mutate(rel_real = round(rel_real, 2)) %>% 
  mutate(rel_llm = round(rel_llm, 2)) %>% 
ggplot(., aes(rel_real, rel_llm, label = scale, color  =  str_detect(scale, "random"))) + 
  geom_abline(linetype = "dashed") +
  geom_point() +
  scale_color_viridis_d(guide = "none", end = 0.8) +
  xlab("Actual reliability") +
  ylab("LLM-estimated reliability") +
  guides(color = "none") +
  coord_fixed(xlim = c(0,1), ylim = c(0,1)) -> p

ggplotly(p)
```


## Subscales
```{r}
cors_llm <- holdout_llm %>% 
  mutate(ScaleA = coalesce(str_c(ScaleA, SubscaleA), ScaleA)) %>% 
  mutate(ScaleB = coalesce(str_c(ScaleB, SubscaleB), ScaleB)) %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = CosineSimilarity) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_llm) <- 1

cors_real <- holdout_real %>% 
  mutate(ScaleA = coalesce(str_c(ScaleA, SubscaleA), ScaleA)) %>% 
  mutate(ScaleB = coalesce(str_c(ScaleB, SubscaleB), ScaleB)) %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = Pearson) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_real) <- 1

scales <- llm_holdout_meta %>% 
  mutate(scale_0 = coalesce(str_c(scale_0, scale_1), scale_0)) %>% 
  drop_na(instrument, scale_0) %>% 
  mutate(scale = str_replace_all(paste(instrument, scale_0), "[^a-zA-Z_0-9]", "_")) %>% 
  group_by(scale) %>% 
  summarise(
    items = list(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>% 
  drop_na()

random_scales <- list()
for(i in 1:30) {
  n_items <- rpois(1, 10)
  random_scales[[i]] <- llm_holdout_meta %>% 
    drop_na(instrument, scale_0) %>% 
    sample_n(n_items) %>%  
    mutate(scale = paste0("random", i)) %>% 
  group_by(scale) %>% 
  summarise(
    items = list(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>% 
  drop_na()
}
random_scales <- bind_rows(random_scales)
scales <- bind_rows(scales, random_scales)

find_reverse_items <- function(rs) {
  # Calculate the mean correlation for each item, excluding the item's correlation with itself.
  mean_correlations <- apply(rs, 1, function(x) mean(x[-which(x == 1)]))

  # Identify items with negative mean correlation
  # You may adjust the threshold according to your specific criteria.
  threshold <- -0.01
  reverse_keyed_items <- names(which(mean_correlations < threshold))

  # Now you know which items are likely to be reverse-coded.  
  reverse_keyed_items
}

find_reverse_items_by_first_item <- function(rs) {
  # negatively correlated with first item
  items <- rs[-1, 1]
  reverse_keyed_items <- names(items)[which(items < 0)]
  reverse_keyed_items
}

reverse_items <- function(rs, reverse_keyed_items) {
  # Reverse the correlations for the reverse-keyed items
  for (item in reverse_keyed_items) {
    # Get the index of the reverse-keyed item
    item_index <- which(rownames(rs) == item)
    
    # Reverse the correlations
    rs[item_index, ] <- rs[item_index, ] * -1
    rs[, item_index] <- rs[, item_index] * -1
    
    # Since the diagonal is the correlation of the item with itself, set it back to 1
    rs[item_index, item_index] <- 1
  }
  rs
}


scales <- scales %>% 
  rowwise() %>% 
  mutate(r_real = list(cors_real[items, items]),
         r_llm = list(cors_llm[items, items])) %>% 
  mutate(reverse_items = list(find_reverse_items_by_first_item(r_real)),
         r_real_rev = list(reverse_items(r_real, reverse_items)),
         r_llm_rev = list(reverse_items(r_llm, reverse_items))) %>% 
  mutate(
    cfa_real = list(cfa(lvn, sample.cov = r_real_rev, sample.nobs = 1000)),
    rel_real = semTools::compRelSEM(cfa_real)) %>%
  mutate(
    cfa_llm = list(cfa(lvn, sample.cov = r_llm_rev, sample.nobs = 1000)),
    rel_llm = semTools::compRelSEM(cfa_llm))

scales <- scales %>% filter(between(as.vector(rel_llm), 0, 1)) %>% filter(between(as.vector(rel_real), 0, 1))
cor.test(scales$rel_llm, scales$rel_real)
# scales %>% filter(scale == "LOT_Optimism") %>% pull(cfa_real) %>% .[[1]] %>% summary
```


```{r}
scales %>% 
  mutate(rel_real = round(rel_real, 2)) %>% 
  mutate(rel_llm = round(rel_llm, 2)) %>% 
ggplot(., aes(rel_real, rel_llm, label = scale)) + 
  geom_abline(linetype = "dashed") +
  geom_point() +
  coord_fixed(xlim = c(0,1), ylim = c(0,1)) -> p

ggplotly(p)
```


## Within-Scale
```{r}
holdout_both <- holdout %>% 
  select(ItemStemIdA, ItemStemIdB, Pearson, CosineSimilarity) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdA = ItemStemId, VariableA = Variable, InstrumentA = instrument, ScaleA = scale_0, SubscaleA = scale_1)) %>% 
  left_join(llm_holdout_meta %>% select(ItemStemIdB = ItemStemId, VariableB = Variable, InstrumentB = instrument, ScaleB = scale_0, SubscaleB = scale_1))


holdout_both %>% 
  mutate(same_scale = if_else(ScaleA == ScaleB & SubscaleA == SubscaleB, 1,0,0),
         same_instrument = if_else(InstrumentA == InstrumentB, 1, 0,0)) %>% 
  group_by(same_scale, same_instrument) %>% 
  summarise(cor(Pearson, CosineSimilarity), n())

holdout_both %>% 
  mutate(same_scale = if_else(ScaleA == ScaleB & SubscaleA == SubscaleB, 1,0,0),
         same_instrument = if_else(InstrumentA == InstrumentB, 1, 0,0)) %>% 
  group_by(same_scale, same_instrument) %>% 
  summarise(cor(abs(Pearson), abs(CosineSimilarity)))

holdout_both %>% 
  mutate(same_scale = if_else(ScaleA == ScaleB & SubscaleA == SubscaleB, 1,0,0),
         same_instrument = if_else(InstrumentA == InstrumentB, 1, 0,0)) %>% 
  group_by(same_scale, same_instrument, ScaleA, SubscaleA, InstrumentA) %>% 
  summarise(cor = cor(Pearson, CosineSimilarity)) %>% 
  group_by(same_scale, same_instrument) %>% 
  summarise(mean(cor, na.rm = T))
```



## Scales
```{r}
cors_llm <- holdout_llm %>% 
  mutate(ScaleA = coalesce(str_c(ScaleA, SubscaleA), ScaleA)) %>% 
  mutate(ScaleB = coalesce(str_c(ScaleB, SubscaleB), ScaleB)) %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = CosineSimilarity) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_llm) <- 1

cors_real <- holdout_real %>% 
  mutate(ScaleA = coalesce(str_c(ScaleA, SubscaleA), ScaleA)) %>% 
  mutate(ScaleB = coalesce(str_c(ScaleB, SubscaleB), ScaleB)) %>% 
  drop_na(InstrumentA, ScaleA, InstrumentB, ScaleB) %>% 
  select(x = VariableA, y = VariableB, r = Pearson) %>% 
  as.data.frame() |> 
  igraph::graph_from_data_frame(directed = FALSE) |> 
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_real) <- 1

scales <- llm_holdout_meta %>% 
  mutate(scale_0 = coalesce(str_c(scale_0, scale_1), scale_0)) %>% 
  drop_na(instrument, scale_0) %>% 
  mutate(scale = str_replace_all(paste(instrument, scale_0), "[^a-zA-Z_0-9]", "_")) %>% 
  group_by(scale) %>% 
  summarise(
    items = list(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>% 
  drop_na()

scales <- scales %>% 
  rowwise() %>% 
  mutate(r_real = list(cors_real[items, items]),
         r_llm = list(cors_llm[items, items])) %>% 
  mutate(reverse_items = list(find_reverse_items_by_first_item(r_real)))

cors_real_rev = reverse_items(cors_real, scales$reverse_items %>% unlist())
cors_llm_rev = reverse_items(cors_llm, scales$reverse_items %>% unlist())


model <- scales$lvn %>% head(2) %>% paste(collapse = "\n")

lcor_real <- cfa(model, 
            sample.cov = cors_real_rev, sample.nobs = 1000)

library(glue)
model_name = "item-similarity-20230710-164559"
holdout_human_data = arrow::read_feather(
  file = glue("ignore.{model_name}.raw.osf-bainbridge-2021-s2-0.human.feather")
)
lcor_real <- cfa(model, data = holdout_human_data)
lcor_llm <- cfa(model, 
            sample.cov = cors_llm_rev, sample.nobs = 1000)

estimated_rs <- standardizedsolution(lcor_real) %>% 
  full_join(standardizedsolution(lcor_llm), by = c("lhs", "op", "rhs")) %>% 
  select(lhs, op, rhs, est.std.x, est.std.y, se.x, se.y) 

lv_rs <- estimated_rs %>% filter(op == "~~") %>% 
  filter(lhs != rhs)


cor.test(lv_rs$est.std.x, lv_rs$est.std.y)
```



```{r}
lv_rs %>% 
  mutate(correlation = round(est.std.x, 2)) %>% 
  mutate(llm_based = round(est.std.y, 2)) %>% 
  mutate(scales = str_c(lhs, "\n", rhs)) %>% 
ggplot(., aes(correlation, llm_based, label = scales)) + 
  geom_abline(linetype = "dashed") +
  geom_point() +
  xlab("Latente Skalenkorrelation, empirisch") +
  ylab("Latente Skalenkorrelation, LLM") + 
  theme_bw() +
  coord_fixed(xlim = c(-1,1), ylim = c(-1,1)) -> p
p
ggplotly(p)
```


## Shuffle on through

```{r}
for(i in 1:30) {
  model <- scales$lvn %>% sample(10) %>% paste(collapse = "\n")
  
  lcor_real <- cfa(model, 
              sample.cov = cors_real_rev, sample.nobs = 1000)
  lcor_llm <- cfa(model, 
              sample.cov = cors_llm_rev, sample.nobs = 1000)
  
  estimated_rs <- standardizedsolution(lcor_real) %>% 
    full_join(standardizedsolution(lcor_llm), by = c("lhs", "op", "rhs")) %>% 
    select(lhs, op, rhs, est.std.x, est.std.y) 
  
  lv_rs <- bind_rows(lv_rs,
                     estimated_rs %>% filter(op == "~~") %>% 
                       filter(lhs != rhs)
  )
}

lv_rsd <- lv_rs %>% distinct(lhs, rhs, .keep_all = T)



cor.test(lv_rsd$est.std.x, lv_rsd$est.std.y)
```

## Reliability
```{r}
## precision analysis for reliability
cors_llm <- holdout_llm %>%
  select(x = VariableA, y = VariableB, r = CosineSimilarity) %>%
  as.data.frame() |>
  igraph::graph_from_data_frame(directed = FALSE) |>
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_llm) <- 1

cors_real <- holdout_llm %>%
  select(x = VariableA, y = VariableB, r = Pearson) %>%
  as.data.frame() |>
  igraph::graph_from_data_frame(directed = FALSE) |>
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_real) <- 1

subscales <- llm_holdout_meta %>%
  select(instrument, scale_0, scale_1, Variable, ItemStemId) %>%
  distinct() %>%
  mutate(instrument = coalesce(str_c(str_trim(instrument), "_"), ""),
         scale_0 = coalesce(str_c(str_trim(scale_0), "_"), ""),
         scale_1 = coalesce(str_trim(scale_1), ""),
         scale = str_replace_all(paste0(instrument, scale_0, scale_1), "[^a-zA-Z_0-9]", "_")
  )
n_distinct(subscales$scale)

scales <- llm_holdout_meta %>%
  select(instrument, scale_0, scale_1, Variable, ItemStemId) %>%
  distinct() %>%
  mutate(instrument = coalesce(str_c(str_trim(instrument), "_"), ""),
         scale_0 = coalesce(str_c(str_trim(scale_0), "_"), ""),
         scale_1 = "",
         scale = str_replace_all(paste0(instrument, scale_0, scale_1), "[^a-zA-Z_0-9]", "_")
  )

scales <- bind_rows(scales, subscales) %>% distinct() %>% filter(scale != "")
n_distinct(scales$scale)

scales <- scales %>%
  group_by(scale) %>%
  summarise(
    items = list(Variable),
    number_of_items = n_distinct(Variable),
    lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>%
  drop_na()

random_scales <- list()
for(i in 1:200) {
  n_items <- rpois(1, 10)
  n_items <- if_else(n_items < 3, 3, n_items)
  random_scales[[i]] <- llm_holdout_meta %>%
    sample_n(n_items) %>%
    mutate(scale = paste0("random", i)) %>%
    group_by(scale) %>%
    summarise(
      items = list(Variable),
      number_of_items = n_distinct(Variable),
      lvn = paste(first(scale), " =~ ", paste(Variable, collapse = " + "))) %>%
    drop_na()
}

random_scales <- bind_rows(random_scales)
scales <- bind_rows(scales, random_scales)
n_distinct(scales$scale)

find_reverse_items_by_first_item <- function(rs) {
  # negatively correlated with first item
  items <- rs[-1, 1]
  reverse_keyed_items <- names(items)[which(items < 0)]
  reverse_keyed_items
}

reverse_items <- function(rs, reverse_keyed_items) {
  # Reverse the correlations for the reverse-keyed items
  for (item in reverse_keyed_items) {
    # Get the index of the reverse-keyed item
    item_index <- which(rownames(rs) == item)

    # Reverse the correlations
    rs[item_index, ] <- rs[item_index, ] * -1
    rs[, item_index] <- rs[, item_index] * -1

    # Since the diagonal is the correlation of the item with itself, set it back to 1
    rs[item_index, item_index] <- 1
  }
  rs
}

scales <- scales %>% filter(number_of_items >= 3)

scales <- scales %>%
  rowwise() %>%
  mutate(r_real = list(cors_real[items, items]),
         r_llm = list(cors_llm[items, items])) %>%
  mutate(reverse_items = list(find_reverse_items_by_first_item(r_real)),
         r_real_rev = list(reverse_items(r_real, reverse_items)),
         r_llm_rev = list(reverse_items(r_llm, reverse_items))) %>%
  mutate(
    rel_real = list(psych::alpha(r_real_rev, keys = F, n.obs = 400)$feldt)) %>%
  mutate(
    rel_llm = list(psych::alpha(r_llm_rev, keys = F, n.obs = 400)$feldt)) %>%
  mutate(rel_real_alpha = rel_real$alpha$raw_alpha,
         rel_llm_alpha = rel_llm$alpha$raw_alpha) %>%
  mutate(
    alpha_se = mean(diff(unlist(psychometric::alpha.CI(rel_real_alpha, k = number_of_items, N = 400, level = 0.95))))
  )

qplot(scales$alpha_se)
qplot(scales$rel_real_alpha, scales$alpha_se)
qplot(scales$number_of_items, scales$alpha_se)
qplot(scales$rel_real_alpha, scales$alpha_se, color = scales$number_of_items)


se2 <- mean(scales$alpha_se^2)

r <- broom::tidy(cor.test(scales$rel_real_alpha, scales$rel_llm_alpha))
  model <- paste0('
    # Latent variables
    PearsonLatent =~ 1*rel_real_alpha

    # Fixing error variances based on known standard errors
    rel_real_alpha ~~ ',se2,'*rel_real_alpha

    # Relationship between latent variables
    PearsonLatent ~~ rel_llm_alpha
  ')

  fit <- sem(model, data = subset)

  results <- standardizedsolution(fit) %>% filter(lhs == "PearsonLatent", rhs ==  "rel_llm_alpha")
results
```

